# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LlH2hpBAHHdjq09Go7hQ3c4CDvfWQh-A

Bitcoin *Predict*

Get Data
"""

#Import

import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv', parse_dates=["Date"],
                 index_col=["Date"]) #programa bunun date olduğunu söylüyoruz.

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv

df.head()

df.info()

len(df) #kaç tane veri var bakalım.

8*365 #8 yılın verisi var

#Her günün sonunda kapanan fiyatları başka bir yerde toplayalım.


bitcoin_prices = pd.DataFrame(df["Closing Price (USD)"]).rename(columns={"Closing Price (USD)": "Price"})
bitcoin_prices.head()

import matplotlib.pyplot as plt

bitcoin_prices.plot(figsize=(12, 8))
plt.ylabel("BTC Price")
plt.title("Price of Bitcoin from 1 Oct 2013 to 18 May 2021", fontsize=16)
plt.legend(fontsize=14);

# CSV Module

#Importing and formatting

import csv
from datetime import datetime

timesteps = []
btc_price = []
with open('/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv', 'r') as f:
  csv_reader = csv.reader(f, delimiter=',') #virgülle sütunları birbirinden ayır
  next(csv_reader) #ilk satırı atla (başlıklardan kurtulduk)
  for line in csv_reader:
    timesteps.append(datetime.strptime(line[1], '%Y-%m-%d')) #tarihi veren hücreye gidiyoruz
    btc_price.append(float(line[2])) #kapanış fiyatını al

timesteps[:10], bitcoin_prices[:10]

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10, 7))
plt.plot(timesteps, btc_price)
plt.title("Bitcoin Fiyatı")
plt.xlabel("Tarih")
plt.ylabel("Fiyat")
plt.show()

"""Train - Test"""

## Date dizisini alalım

btc_price[:5]

timesteps = bitcoin_prices.index.to_numpy()
prices = bitcoin_prices["Price"].to_numpy()

timesteps[:10], prices[:10]

#from sklearn.model_selection import train_test_split

#X_train, X_test, y_train, y_test = train_test_split(timesteps,
  #                                                  prices,
    #                                                test_size=0.2,
     #                                               random_state=42)

#len(X_train), len(X_test), len(y_train), len(y_test)

#Zaman serisi için datayı bölelim (%80 train, %20 test)

split_size = int(0.8 * len(prices))

#Train data (ikiye ayrılan datanın ilk kısmı)

X_train, y_train = timesteps[:split_size], prices[:split_size]

#Test data (ikinci kısmı)

X_test, y_test = timesteps[split_size:], prices[split_size:]

len(X_train), len(X_test), len(y_train), len(y_test)

plt.figure(figsize=(10, 7))
plt.scatter(X_train, y_train, c="b", label="Train data")
plt.scatter(X_test, y_test, c="r", label="Test data")
plt.xlabel("Tarih")
plt.ylabel("Fiyat")
plt.legend();

# Plot fonksiyonu yazalım tekrar tekrar yazmayalım

def plot_time_series(timesteps,values,format=".",start=0, end=None, label=None):
   #start mavi kısım

   plt.plot(timesteps[start:end], values[start:end], format, label=label)
   plt.xlabel("Tarih")
   plt.ylabel("Fiyat")
   if label:
     plt.legend(fontsize=14) #legend
   plt.grid(True)

plt.figure(figsize=(10,7))
plot_time_series(timesteps=X_train, values=y_train, label="Train Data")
plot_time_series(timesteps=X_test, values=y_test, label="Test Data")

"""Modelleme ve Model Karşılaştırması


1.   Naive Model
2.   Dense Model
3.   Conv1D
4.   LSTM
5.   N-BEATS Algortihm
6.   Ensemble
7.   Future Prediction





*   Horizon : Tahmin edeceğimiz gelecekteki zaman adımlarının sayısı

*   Window Size : Tahmin için kullanacağımız zaman adımları

1. Naive Metodu
"""

y_test[:10]

naive_forecast = y_test[:-1]
naive_forecast[:10], naive_forecast[-10:]

y_test[-10:]

"""Son hücrelere bakarsak, gerçekte olan veri: 43144.47129086 ve naive yönteminin tahmini: 45604.61575361"""

plt.figure(figsize=(10,7))
#plot_time_series(timesteps=X_train, values=y_train,  label="Train data")
plot_time_series(timesteps=X_test, values=y_test, start=350, format="-", label="Test data")
plot_time_series(timesteps=X_test[1:], values=naive_forecast, start=350, format="-", label="Naive forecast") #ilk indexi alırsak shape error verir

"""Naive metodu nerdeyse aynı değerleri tahmin ediyor."""

import tensorflow as tf

#MASE uygulaması

def mean_absolute_scaled_error(y_true,y_pred):

  mae = tf.reduce_mean(tf.abs(y_true - y_pred))

  #Naive Tahmininin MAE'sini bul

  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) #mevsimsellik 1 gün. 1 günde 1 veri var.

  return mae/mae_naive_no_season

  #MASE'yi hesapla

  return mae/mae_naive_no_season

mean_absolute_scaled_error(y_true=y_test[1:],y_pred=naive_forecast).numpy()

"""Sonuç<1 olduğu için tahminleme naiveden daha iyi

Model tahminlerinin gerçek değerleri ve değerlendirme metriklerinin geri döndürülmesi
"""

def evaluate_preds(y_true, y_pred):
  #metrik hesaplaması için verilerin float32 tipinde olması lazım
  #y_true = tf.cast(y_true, y_pred.dtype)
  #y_pred = tf.cast(y_pred, y_pred.dtype)

  y_true = tf.cast(y_true, dtype=tf.float32)
  y_pred = tf.cast(y_pred, dtype=tf.float32)

  #Metrikler

  mea = tf.keras.metrics.mean_absolute_error(y_true, y_pred)
  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)
  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)
  rmse = tf.sqrt(mse)
  mase = mean_absolute_scaled_error(y_true, y_pred)

  return {"mae":mea.numpy(), "mse":mse.numpy(), "mape":mape.numpy(), "rmse":rmse.numpy(), "mase":mase.numpy()}

naive_results = evaluate_preds(y_true=y_test[1:], y_pred=naive_forecast)
naive_results

"""Her zaman düşük sonucu veren daha iyidir."""

naive_forecast[-10:]

"""mae'ye göre tahmin edilen fiyatlar 567 dolar kadar fark ediyor"""

tf.reduce_mean(y_test)

tf.reduce_min(y_test), tf.reduce_max(y_test)

"""max değer (63346) için mae değeri kabul edilebilir olsa da minimum değer için kötü bir oran.

Window : Zaman serisi problemlerimizi denetimli öğrenme problemlerine dönüştürmek için window kullanıyoruz.
"""

##Format veri 2
#[0,1,2,3,4,5] -> [6]

y_train

len(y_train)

btc_price[:7], btc_price[7]

print(f"Kullanmak istediğimiz data: {btc_price[:7]} Tahmin edeceğimiz data: {btc_price[7]}")

HORIZON = 1
WINDOW_SIZE = 7

import numpy as np
def get_labelled_window(x, horizon=HORIZON):
  #Horizon=1 ise
  # Input = [0,1,2,3,4,5,6,7] -> Output= ([0,1,2,3,4,5,6],[7])
  return x[:, :-horizon], x[:, -horizon]

test_window, test_label = get_labelled_window(tf.expand_dims(tf.range(8), axis=0))
test_window, test_label

tf.expand_dims(tf.range(8)+1, axis=0)

print(f"Window: {tf.squeeze(test_window).numpy()} --> d: {tf.squeeze(test_label).numpy()}")

#Yukardaki yaptığımızı şuanki daha büyük veri için uygulamalıyız.
#Numpy array indexing
'''
[[0,1,2,3,4,5,6],
[1,2,3,4,5,6,7],
[2,3,4,5,6,7,8]]
'''
def make_windows(x, window_size = WINDOW_SIZE, horizon = HORIZON):
  # 2 boyutlu diziden 1 boyutlu dizi döndürecek
  # 1. window size büyüklüğünde dizinin sonuna horizon ekle
  window_step = np.expand_dims(np.arange(window_size + horizon),axis=0)

  #2. Indexing
  window_indexes = window_step + np.expand_dims(np.arange(len(x) - (window_size+horizon-1)),axis=0).T

  #print(print(f"Window indexes:\n {window_indexes, window_indexes.shape} "))

  #3 her bir diziyi alıp prices 'a yazacağız
  windowed_array = x[window_indexes]
  #print(f"Windowed array:\n {windowed_array, windowed_array.shape}")

  #4.Etiketlenmiş window'ları al
  windows, labels = get_labelled_window(windowed_array, horizon=horizon)


  return windows, labels

full_windows, full_labels = make_windows(prices,window_size=WINDOW_SIZE, horizon=HORIZON)
len(full_windows), len(full_labels)

len(prices)

for i in range(3):
  print(f"Window: {full_windows[i]} --> Horizon: {full_labels[i]}")

#Train / Test splits

def make_train_test_splits(windows, labels, test_split = 0.2):
  splits_size = int(len(windows)* (1-test_split)) #%80 train , %20 test
  train_windows = windows[:splits_size]
  train_labels = labels[:splits_size]
  test_windows = windows[splits_size:]
  test_labels = labels[splits_size:]
  return train_windows, test_windows, train_labels, test_labels

train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)
len(train_windows), len(test_windows), len(train_labels), len(test_labels)

len(full_windows)*0.8

train_windows[:5], test_label[:5]

y_train

#windowlama yapmadan önceki test_label ile şuanki test_label'ı karşılaştıralım

np.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:])

#ModelCheckPoint

import os

def create_model_checkpoint(model_name, save_path = "model_expriments"):
  return tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join(save_path, model_name),
    verbose=0,
    save_best_only =True)

#Model 1 = Dense Model (Window = 7, Horizon = 1)
# 1 output , 128 hidden unit, ReLU
#Linear Activasyon
#Adam optimizasyonu ve MAE kayıp fonksiyonu
#Batch_size = 128
#100 epochs

import tensorflow as tf
from tensorflow.keras import layers

tf.random.set_seed(42)

#Modeli kur
model_1 = tf.keras.Sequential([
  layers.Dense(128, activation="relu"),
  layers.Dense(HORIZON, activation="linear")
], name = "Model_1_dense")

#Compile
model_1.compile(loss="mae", optimizer=tf.keras.optimizers.Adam(), metrics=["mae","mse"])

#Modeli Fit Et

model_1.fit(x=train_windows,
            y=train_labels,
            epochs=100,
            verbose=1,
            batch_size=128,
            validation_data=(test_windows,test_labels),
            callbacks=[create_model_checkpoint(model_name=model_1.name)])

model_1.evaluate(test_windows, test_labels)

model_1 = tf.keras.models.load_model("model_expriments/Model_1_dense")
model_1.evaluate(test_windows, test_labels)

naive_results

#Test data üzerinde model ile tahminleme yapalım

"""1. Eğitilmiş modeli al
2. Input data al
3. Input'ları predict() modeline sok
4. Tahmini geri döndür
"""

def make_preds(model, input_data):
  forecast = model.predict(input_data)
  return tf.squeeze(forecast) #1D boyutlu dizi döndür

#Model_1'i kullanarak tahmin yap.

model_1_preds = make_preds(model_1, test_windows)
len(model_1_preds), model_1_preds[:10]

test_labels[:10]

test_labels.shape, model_1_preds.shape

tf.squeeze(test_labels).shape, model_1_preds.shape

model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels),
                                 y_pred=model_1_preds)
model_1_results

naive_results

offset = 450
plt.figure(figsize=(10, 7))


plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=model_1_preds,
                 start=offset,
                 format="-",
                 label="model_1_preds")


plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=test_labels[:],
                 start=offset,
                 format="-",
                 label="Test Data")

"""Kaymaların olmasının sebebi bunun aslında tahmin sonuçları değil test data olmasıdır.

Model 2 : Dense (window030, horizon=1)
"""

HORIZON = 1 # bir zamanda 1 adımı tahmin et.
WINDOW_SIZE = 30 # geçmişteki 30 adım

full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)
len(full_windows), len(full_labels)

#Train, test windows
train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels)
len(train_windows), len(test_windows), len(train_labels), len(test_labels)

#Modeli kuralım

model_2 = tf.keras.Sequential([
  layers.Dense(128, activation="relu"),
  layers.Dense(HORIZON)
], name="model_2_dense")

#compile
model_2.compile(loss="mae",
                optimizer=tf.keras.optimizers.Adam())
#fit
model_2.fit(train_windows,
            train_labels,
            epochs=100,
            batch_size=128,
            verbose=0,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_2.name)])

model_2.evaluate(test_windows, test_labels)

#en iyi modeli kaydet

model_2 = tf.keras.models.load_model("model_experiments/model_2_dense/")
model_2.evaluate(test_windows, test_labels)

model_2_preds = make_preds(model_2,
                           input_data=test_windows)

model_2_results = evaluate_preds(y_true=tf.squeeze(test_labels), # remove 1 dimension of test labels
                                 y_pred=model_2_preds)
model_2_results

naive_results

"""Naive modelden daha iyi çalışmıyor model2.
O zaman daha küçük window size daha iyi sonuçlar çıkarıyor.
"""

offset = 300
plt.figure(figsize=(10, 7))
# Account for the test_window offset
plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:,], start=offset, label="test_data")
plot_time_series(timesteps=X_test[-len(test_windows):], values=model_2_preds, start=offset, format="-", label="model_2_preds")

"""Model 3: Dense(window=30, horizon=7)"""

HORIZON = 7
WINDOW_SIZE = 30

full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)
len(full_windows), len(full_labels)

train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels, test_split=0.2)
len(train_windows), len(test_windows), len(train_labels), len(test_labels)

tf.random.set_seed(42)

#Modeli kur
model_3 = tf.keras.Sequential([
  layers.Dense(128, activation="relu"),
  layers.Dense(HORIZON)
], name="model_3_dense")

model_3.compile(loss="mae",
                optimizer=tf.keras.optimizers.Adam())

model_3.fit(train_windows,
            train_labels,
            batch_size=128,
            epochs=100,
            verbose=0,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_3.name)])

model_3.evaluate(test_windows, test_labels)

model_3 = tf.keras.models.load_model("model_experiments/model_3_dense/")
model_3.evaluate(test_windows, test_labels)

model_3_preds = make_preds(model_3,
                           input_data=test_windows)
model_3_preds[:5]

model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),
                                 y_pred=model_3_preds)
model_3_results

def evaluate_preds(y_true, y_pred):
  # Make sure float32 (for metric calculations)
  y_true = tf.cast(y_true, dtype=tf.float32)
  y_pred = tf.cast(y_pred, dtype=tf.float32)

  # Calculate various metrics
  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)
  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)
  rmse = tf.sqrt(mse)
  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)
  mase = mean_absolute_scaled_error(y_true, y_pred)

  # Account for different sized metrics (for longer horizons, reduce to single number)
  if mae.ndim > 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean
    mae = tf.reduce_mean(mae)
    mse = tf.reduce_mean(mse)
    rmse = tf.reduce_mean(rmse)
    mape = tf.reduce_mean(mape)
    mase = tf.reduce_mean(mase)

  return {"mae": mae.numpy(),
          "mse": mse.numpy(),
          "rmse": rmse.numpy(),
          "mape": mape.numpy(),
          "mase": mase.numpy()}

model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),
                                 y_pred=model_3_preds)
model_3_results

offset = 300
plt.figure(figsize=(10, 7))
plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, ], start=offset, label="Test_data")
plot_time_series(timesteps=X_test[-len(test_windows):], values=model_3_preds, start=offset, label="model_3_preds")

offset = 300
plt.figure(figsize=(10, 7))

plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=test_labels[:, ],
                 start=offset,
                 label="Test_data")
plot_time_series(timesteps=X_test[-len(test_windows):],
                 values=tf.reduce_mean(model_3_preds, axis=1),
                 format="-",
                 start=offset,
                 label="model_3_preds")

"""Modelleri Karşılaştıralım"""

pd.DataFrame({"naive": naive_results["mae"],
              "horizon_1_window_7": model_1_results["mae"],
              "horizon_1_window_30": model_2_results["mae"],
              "horizon_7_window_30": model_3_results["mae"]}, index=["mae"]).plot(figsize=(10, 7), kind="bar");

"""Horizon=1 ve window=7, Naive metodunun sonuçlarına en yakın olan model oldu.

Model 4: Conv1D
"""

HORIZON = 1 # bir sonraki günü tahmin et
WINDOW_SIZE = 7 # geçen haftanın verilerini al

# window veri seti oluştur
full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)
len(full_windows), len(full_labels)

# train/test splits
train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)
len(train_windows), len(test_windows), len(train_labels), len(test_labels)

train_windows[0].shape # returns (WINDOW_SIZE, )

x = tf.constant(train_windows[0])
expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))
print(f"Original shape: {x.shape}") # (WINDOW_SIZE)
print(f"Expanded shape: {expand_dims_layer(x).shape}") # (WINDOW_SIZE, input_dim)
print(f"Original values with expanded shape:\n {expand_dims_layer(x)}")

tf.random.set_seed(42)

# model oluştur
model_4 = tf.keras.Sequential([

  layers.Lambda(lambda x: tf.expand_dims(x, axis=1)),
  layers.Conv1D(filters=128, kernel_size=5, padding="causal", activation="relu"),
  layers.Dense(HORIZON)
], name="model_4_conv1D")

# Compile model
model_4.compile(loss="mae",
                optimizer=tf.keras.optimizers.Adam())

# Fit model
model_4.fit(train_windows,
            train_labels,
            batch_size=128,
            epochs=100,
            verbose=0,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_4.name)])

model_4.summary()

model_4 = tf.keras.models.load_model("model_experiments/model_4_conv1D")
model_4.evaluate(test_windows, test_labels)

# Tahminleme
model_4_preds = make_preds(model_4, test_windows)
model_4_preds[:10]

# Tahminleri değerlendir
model_4_results = evaluate_preds(y_true=tf.squeeze(test_labels),
                                 y_pred=model_4_preds)
model_4_results

"""Model 5 : RNN (LSTM)"""

tf.random.set_seed(42)


inputs = layers.Input(shape=(WINDOW_SIZE))
x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs)
# print(x.shape)
# x = layers.LSTM(128, activation="relu", return_sequences=True)(x)
x = layers.LSTM(128, activation="relu")(x)
# print(x.shape)

# x = layers.Dense(32, activation="relu")(x)
output = layers.Dense(HORIZON)(x)
model_5 = tf.keras.Model(inputs=inputs, outputs=output, name="model_5_lstm")

# Compile model
model_5.compile(loss="mae",
                optimizer=tf.keras.optimizers.Adam())


model_5.fit(train_windows,
            train_labels,
            epochs=100,
            verbose=0,
            batch_size=128,
            validation_data=(test_windows, test_labels),
            callbacks=[create_model_checkpoint(model_name=model_5.name)])

model_5 = tf.keras.models.load_model("model_experiments/model_5_lstm/")
model_5.evaluate(test_windows, test_labels)

#Tahminleme
model_5_preds = make_preds(model_5, test_windows)
model_5_preds[:10]

#Tahminleri değerlendir
model_5_results = evaluate_preds(y_true=tf.squeeze(test_labels),
                                 y_pred=model_5_preds)
model_5_results

"""Görünüşe göre model4 ve model5 naive modelini geçemedi.

Çok Değişkenli Zaman Serisi
"""

bitcoin_prices.head()

# Block reward values
block_reward_1 = 50 # 3 January 2009 (2009-01-03) - bu block reward veri setinde yazmıyor
block_reward_2 = 25 # 28 November 2012
block_reward_3 = 12.5 # 9 July 2016
block_reward_4 = 6.25 # 11 May 2020

# Block reward tarihleri
block_reward_2_datetime = np.datetime64("2012-11-28")
block_reward_3_datetime = np.datetime64("2016-07-09")
block_reward_4_datetime = np.datetime64("2020-05-11")

block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days
block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days
block_reward_2_days, block_reward_3_days

#  block_reward kolonu ekle
bitcoin_prices_block = bitcoin_prices.copy()
bitcoin_prices_block["block_reward"] = None

# verileri sütuna gir
bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2
bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3
bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4
bitcoin_prices_block.head()

from sklearn.preprocessing import minmax_scale
scaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[["Price", "block_reward"]]), # we need to scale the data first
                                     columns=bitcoin_prices_block.columns,
                                     index=bitcoin_prices_block.index)
scaled_price_block_df.plot(figsize=(10, 7));

"""Block reward price aşağı indikçe bitcoin fiyatı artıyor. Modellerimizin daha iyi sonuç bulmasına yardım edebilir."""

HORIZON = 1
WINDOW_SIZE = 7

#datayı kopyala
bitcoin_prices_windowed = bitcoin_prices_block.copy()

# window kolonlarını ekle
for i in range(WINDOW_SIZE):
  bitcoin_prices_windowed[f"Price+{i+1}"] = bitcoin_prices_windowed["Price"].shift(periods=i+1)
bitcoin_prices_windowed.head(10)

"""


*   [0, 1, 2, 3, 4, 5, 6, block_reward] -> [7]
*   [1, 2, 3, 4, 5, 6, 7, block_reward] -> [8]


"""

#NaN değerleri kaldıralım
X = bitcoin_prices_windowed.dropna().drop("Price", axis=1).astype(np.float32)
y = bitcoin_prices_windowed.dropna()["Price"].astype(np.float32)
X.head()

y.head()

#Train/Test

split_size = int(len(X) * 0.8)
X_train, y_train = X[:split_size], y[:split_size]
X_test, y_test = X[split_size:], y[split_size:]
len(X_train), len(y_train), len(X_test), len(y_test)

"""Model 6 : Dense (Çokdeğişkenli Zaman Serileri)"""

tf.random.set_seed(42)

# Make multivariate time series model
model_6 = tf.keras.Sequential([
  layers.Dense(128, activation="relu"),
  # layers.Dense(128, activation="relu"), # adding an extra layer here should lead to beating the naive model
  layers.Dense(HORIZON)
], name="model_6_dense_multivariate")

# Compile
model_6.compile(loss="mae",
                optimizer=tf.keras.optimizers.Adam())

# Fit
model_6.fit(X_train, y_train,
            epochs=100,
            batch_size=128,
            verbose=0, # only print 1 line per epoch
            validation_data=(X_test, y_test),
            callbacks=[create_model_checkpoint(model_name=model_6.name)])

model_6 = tf.keras.models.load_model("model_experiments/model_6_dense_multivariate")
model_6.evaluate(X_test, y_test)

model_6_preds = tf.squeeze(model_6.predict(X_test))
model_6_preds[:10]

#Tahminler
model_6_results = evaluate_preds(y_true=y_test,
                                 y_pred=model_6_preds)
model_6_results

model_1_results

"""Sonuçlar biraz daha iyiye gitse de model_1'in tahmin sonuçlarına yaklaşamadı.

Model 7 : N_BEATS Algoritması

Sonuçları geliştirmek için katmanların sayısını arttırmak bir yöntem olabilir. N_BEATS algoritması bu şekilde çalışıyor.

N-Beats Block Layer
"""

class NBeatsBlock(tf.keras.layers.Layer):
  def __init__(self, # the constructor takes all the hyperparameters for the layer
               input_size: int,
               theta_size: int,
               horizon: int,
               n_neurons: int,
               n_layers: int,
               **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)
    super().__init__(**kwargs)
    self.input_size = input_size
    self.theta_size = theta_size
    self.horizon = horizon
    self.n_neurons = n_neurons
    self.n_layers = n_layers

    # Block contains stack of 4 fully connected layers each has ReLU activation
    self.hidden = [tf.keras.layers.Dense(n_neurons, activation="relu") for _ in range(n_layers)]
    # Output of block is a theta layer with linear activation
    self.theta_layer = tf.keras.layers.Dense(theta_size, activation="linear", name="theta")

  def call(self, inputs): # the call method is what runs when the layer is called
    x = inputs
    for layer in self.hidden: # pass inputs through each hidden layer
      x = layer(x)
    theta = self.theta_layer(x)
    # Output the backcast and forecast from theta
    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]
    return backcast, forecast

#girdi ve çıktıları temsil etmek için geçici bir layer oluşturalım
dummy_nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE,
                                       theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast
                                       horizon=HORIZON,
                                       n_neurons=128,
                                       n_layers=4)

#Geçici input oluşturalım
dummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE) + 1, axis=0) # input shape to the model has to reflect Dense layer input requirements (ndim=2)
dummy_inputs

# Geçiçi girdileri geçici layer'a verelim
backcast, forecast = dummy_nbeats_block_layer(dummy_inputs)

print(f"Backcast: {tf.squeeze(backcast.numpy())}")
print(f"Forecast: {tf.squeeze(forecast.numpy())}")

#Algoritma içn veriyi hazırlayalım

HORIZON = 1
WINDOW_SIZE = 7

bitcoin_prices.head()

bitcoin_prices_nbeats = bitcoin_prices.copy()
for i in range(WINDOW_SIZE):
  bitcoin_prices_nbeats[f"Price+{i+1}"] = bitcoin_prices_nbeats["Price"].shift(periods=i+1)
bitcoin_prices_nbeats.dropna().head()

X = bitcoin_prices_nbeats.dropna().drop("Price", axis=1)
y = bitcoin_prices_nbeats.dropna()["Price"]

#  train-test
split_size = int(len(X) * 0.8)
X_train, y_train = X[:split_size], y[:split_size]
X_test, y_test = X[split_size:], y[split_size:]
len(X_train), len(y_train), len(X_test), len(y_test)

# 1. Turn train and test arrays into tensor Datasets
train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)
train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)

test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)
test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)

# 2. Combine features & labels
train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))
test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))

# 3. Batch and prefetch for optimal performance
BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper
train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

train_dataset, test_dataset

# Values from N-BEATS paper Figure 1 and Table 18/Appendix D
N_EPOCHS = 5000 # called "Iterations" in Table 18
N_NEURONS = 512 # called "Width" in Table 18
N_LAYERS = 4
N_STACKS = 30

INPUT_SIZE = WINDOW_SIZE * HORIZON # called "Lookback" in Table 18
THETA_SIZE = INPUT_SIZE + HORIZON

INPUT_SIZE, THETA_SIZE

#  tensor oluştur
tensor_1 = tf.range(10) + 10
tensor_2 = tf.range(10)

# Çıkar
subtracted = layers.subtract([tensor_1, tensor_2])

# Ekle
added = layers.add([tensor_1, tensor_2])

print(f"Input tensors: {tensor_1.numpy()} & {tensor_2.numpy()}")
print(f"Subtracted: {subtracted.numpy()}")
print(f"Added: {added.numpy()}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# tf.random.set_seed(42)
# 
# # 1.  N-BEATS Block katmanlarını oluştur
# nbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,
#                                  theta_size=THETA_SIZE,
#                                  horizon=HORIZON,
#                                  n_neurons=N_NEURONS,
#                                  n_layers=N_LAYERS,
#                                  name="InitialBlock")
# 
# # 2. Yığınlara girdileri koy
# stack_input = layers.Input(shape=(INPUT_SIZE), name="stack_input")
# 
# # 3. İlk tahminleri oluştur
# backcast, forecast = nbeats_block_layer(stack_input)
# # Add in subtraction residual link, thank you to: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/174
# residuals = layers.subtract([stack_input, backcast], name=f"subtract_00")
# 
# # 4. Blokları oluştur
# for i, _ in enumerate(range(N_STACKS-1)):
# 
#   # 5. NBeatsBlock kullanarak geriye dönük tahminleri hesapla
#   backcast, block_forecast = NBeatsBlock(
#       input_size=INPUT_SIZE,
#       theta_size=THETA_SIZE,
#       horizon=HORIZON,
#       n_neurons=N_NEURONS,
#       n_layers=N_LAYERS,
#       name=f"NBeatsBlock_{i}"
#   )(residuals)
# 
#   # 6. Geriye dönük tahmin katmanlarını çıkar, gelecek tahmin katmanlarını ekle
#   residuals = layers.subtract([residuals, backcast], name=f"subtract_{i}")
#   forecast = layers.add([forecast, block_forecast], name=f"add_{i}")
# 
# # 7. Modeli birleştir
# model_7 = tf.keras.Model(inputs=stack_input,
#                          outputs=forecast,
#                          name="model_7_N-BEATS")
# 
# # 8.  MAE loss ve Adam optimizer
# model_7.compile(loss="mae",
#                 optimizer=tf.keras.optimizers.Adam(0.001),
#                 metrics=["mae", "mse"])
# 
# # 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks
# model_7.fit(train_dataset,
#             epochs=N_EPOCHS,
#             validation_data=test_dataset,
#             verbose=0,
#             # callbacks=[create_model_checkpoint(model_name=stack_model.name)]
#             callbacks=[tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=200, restore_best_weights=True),
#                       tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", patience=100, verbose=1)])

model_7.evaluate(test_dataset)

model_7_preds = make_preds(model_7, test_dataset)
model_7_preds[:10]

model_7_results = evaluate_preds(y_true=y_test,
                                 y_pred=model_7_preds)
model_7_results

"""1 saatlik bir runtimedan sonra bile, Naive Model kadar iyi bir sonuç çıkaramadı"""

from tensorflow.keras.utils import plot_model
plot_model(model_7)

#Modelleri birleştirelim

def get_ensemble_models(horizon=HORIZON,
                        train_data=train_dataset,
                        test_data=test_dataset,
                        num_iter=10,
                        num_epochs=100,
                        loss_fns=["mae", "mse", "mape"]):


  # boş liste
  ensemble_models = []


  for i in range(num_iter):

    for loss_function in loss_fns:
      print(f"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}")


      model = tf.keras.Sequential([

        layers.Dense(128, kernel_initializer="he_normal", activation="relu"),
        layers.Dense(128, kernel_initializer="he_normal", activation="relu"),
        layers.Dense(HORIZON)
      ])


      model.compile(loss=loss_function,
                    optimizer=tf.keras.optimizers.Adam(),
                    metrics=["mae", "mse"])

      # Fit model
      model.fit(train_data,
                epochs=num_epochs,
                verbose=0,
                validation_data=test_data,

                callbacks=[tf.keras.callbacks.EarlyStopping(monitor="val_loss",
                                                            patience=200,
                                                            restore_best_weights=True),
                           tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss",
                                                                patience=100,
                                                                verbose=1)])


      ensemble_models.append(model)

  return ensemble_models # eğitilmiş modelleri döndür

ensemble_models = get_ensemble_models(num_iter=5,
                                      num_epochs=1000)

"""Ensemble Modeli ile Tahminleme"""

def make_ensemble_preds(ensemble_models, data):
  ensemble_preds = []
  for model in ensemble_models:
    preds = model.predict(data)
    ensemble_preds.append(preds)
  return tf.constant(tf.squeeze(ensemble_preds))

ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models,
                                     data=test_dataset)
ensemble_preds

ensemble_results = evaluate_preds(y_true=y_test,
                                  y_pred=np.median(ensemble_preds, axis=0)) # take the median across all ensemble predictions
ensemble_results

"""Kullandığımız modelelrden en iyi sonucu bu model verdi"""

def get_upper_lower(preds):
  std = tf.math.reduce_std(preds, axis=0)


  interval = 1.96 * std # https://en.wikipedia.org/wiki/1.96


  preds_mean = tf.reduce_mean(preds, axis=0)
  lower, upper = preds_mean - interval, preds_mean + interval
  return lower, upper


lower, upper = get_upper_lower(preds=ensemble_preds)

ensemble_median = np.median(ensemble_preds, axis=0)


offset=500
plt.figure(figsize=(10, 7))
plt.plot(X_test.index[offset:], y_test[offset:], "g", label="Test Data")
plt.plot(X_test.index[offset:], ensemble_median[offset:], "k-", label="Ensemble Median")
plt.xlabel("Date")
plt.ylabel("BTC Price")
plt.fill_between(X_test.index[offset:],
                 (lower)[offset:],
                 (upper)[offset:], label="Prediction Intervals")
plt.legend(loc="upper left", fontsize=14);

"""Model 9 : Geçmiş verilerle geleceği tahmin etme"""

bitcoin_prices_windowed.head()

X_all = bitcoin_prices_windowed.drop(["Price", "block_reward"], axis=1).dropna().to_numpy() # only want prices, our future model can be a univariate model
y_all = bitcoin_prices_windowed.dropna()["Price"].to_numpy()

# 1. Turn X and y into tensor Datasets
features_dataset_all = tf.data.Dataset.from_tensor_slices(X_all)
labels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)

# 2. Combine features & labels
dataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))

# 3. Batch and prefetch for optimal performance
BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper
dataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

dataset_all

tf.random.set_seed(42)

# Create model
model_9 = tf.keras.Sequential([
  layers.Dense(128, activation="relu"),
  layers.Dense(128, activation="relu"),
  layers.Dense(HORIZON)
])

# Compile
model_9.compile(loss=tf.keras.losses.mae,
                optimizer=tf.keras.optimizers.Adam())


model_9.fit(dataset_all,
            epochs=100,
            verbose=0)

INTO_FUTURE = 14

def make_future_forecast(values, model, into_future, window_size=WINDOW_SIZE) -> list:


  future_forecast = []
  last_window = values[-WINDOW_SIZE:]

  for _ in range(into_future):


    future_pred = model.predict(tf.expand_dims(last_window, axis=0))
    print(f"Predicting on: \n {last_window} -> Prediction: {tf.squeeze(future_pred).numpy()}\n")


    future_forecast.append(tf.squeeze(future_pred).numpy())
    # print(future_forecast)


    last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]

  return future_forecast

future_forecast = make_future_forecast(values=y_all,
                                       model=model_9,
                                       into_future=INTO_FUTURE,
                                       window_size=WINDOW_SIZE)

future_forecast[:10]

def get_future_dates(start_date, into_future, offset=1):

  start_date = start_date + np.timedelta64(offset, "D")
  end_date = start_date + np.timedelta64(into_future, "D")
  return np.arange(start_date, end_date, dtype="datetime64[D]")

last_timestep = bitcoin_prices.index[-1]
last_timestep

next_time_steps = get_future_dates(start_date=last_timestep,
                                   into_future=INTO_FUTURE)
next_time_steps

next_time_steps = np.insert(next_time_steps, 0, last_timestep)
future_forecast = np.insert(future_forecast, 0, btc_price[-1])
next_time_steps, future_forecast

plt.figure(figsize=(10, 7))
plot_time_series(bitcoin_prices.index, btc_price, start=2500, format="-", label="Actual BTC Price")
plot_time_series(next_time_steps, future_forecast, format="-", label="Predicted BTC Price")

